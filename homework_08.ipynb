{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713e1293",
   "metadata": {
    "colab_type": "text",
    "id": "b-eNSDPndfKU"
   },
   "source": [
    "### 1. Можно ли отобрать наиболее значимые признаки из имеющихся с помощью PCA? Ответ объясните."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f029e",
   "metadata": {},
   "source": [
    "Ответ: Нет, нельзя. Потому что <font color=\"magenta\">*__метод PCA не осущетсвляет отбор признаков. Метод PCA создает новые признаки,__*</font> которые теряют свою предметную интерпритацию.\n",
    "\n",
    "Features матрицы \"объекты-признаки\" образуют ортогональный базис пространства. Изначально это понятные сущности: длина, ширина чашелистиков, лепестков, цены и т.д.\n",
    "\n",
    "Метод PCA строит (создает) новый базис этого пространства на основе собственных векторов - этот новый базис, к слову сказать, вовсе не обязан даже быть ортогональным, т.е. собственные вектора исходной матрицы \"объекты-признаки\" не обязательно будут ортогональные друг другу (во всяком случае линейной алгебре допускаются ситуации когда собственные вектора неортогональны друг другу, или их может оказаться недостаточно чтобы построить базис пространства, или их может не быть вовсе, или их будет бесконечное множество).\n",
    "\n",
    "Таким образом после перехода к новому базису на основе собственных векторов новые признаки становится невозможно связать с сущностями из окружающего нас мира. Новые признаки это просто feature1, feature2 и т.д. - это максимум что мы можем сделать для ниж. Потом из этого пока что полного набора признаков выбирается подмножество признаков (соответствующих максимальным собственным значениям) и на их основе строится новое подпространство."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18139e",
   "metadata": {
    "colab_type": "text",
    "id": "b-eNSDPndfKU"
   },
   "source": [
    "#### 2. (\\*) Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции [numpy.linalg.svd()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html). Применить к данным на уроке и сравнить ответы."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0db58693",
   "metadata": {},
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4000f943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.facecolor\"] = \"green\"\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3d8e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
       "       [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
       "       [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
       "       [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ],\n",
       "       [-0.53717756,  1.93979142, -1.16971425, -1.05217993]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# стандартизируем признаки\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_ = scaler.fit_transform(X)\n",
    "X_[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b306c",
   "metadata": {},
   "source": [
    "Что мы делаем:\n",
    "\n",
    "Для начала нам нужно найти собственные значения и собственные вектора матрицы ковариации отмаштабированной матрицы \"объекты-признаки\" `X_`. Собственные вектора (и собственные значения) можно найти либо прямым образом - как мы делали на уроке, либо используя сингулярное разложение исходной (маштабированной) матрицы \"объекты-признаки\" `X_`.\n",
    "\n",
    "Мы знаем, что собственные вектора матрицы ковариаций $X\\_^T \\cdot X\\_$ являются правые сингулярные векторы отмаштабироанной матрицы $X\\_$, т.е. столбцы матрицы $V$ из формулы сингулярного разложения:\n",
    "$$A = UDV^T$$\n",
    "а собственные значения матрицы ковариаций будут квадраты сингулярных чисел матрицы $X\\_$\n",
    "\n",
    "Таким образом нам нужно получить сингулярное разложение матирцы $X\\_$ и отобрать столбцы матрицы $V$ соответствующие наибольшим квадратам сингулярных чисел - мы составим из этих столбцов матрицу $W^T$;\n",
    "\n",
    "Чтобы получить новую матрицу \"объекты-признаки\" $Z$ - выполним матричное умножение:\n",
    "$$Z=X\\_W^T.$$\n",
    "\n",
    "вуаля, все должно сойтись с тем что было на уроке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f116e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполняем сингулярное разложение с помощью numpy.linalg\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "u, s, vh = np.linalg.svd(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d728b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем диагональную матрицу D\n",
    "D = np.zeros_like(X_)\n",
    "ind = list(range(4))\n",
    "D[ind, ind] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d603606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получим из матрицы сингулярного разложения VT матрицу V транспонируя VT;\n",
    "v = vh.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb23c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9007,  1.019 , -1.3402, -1.3154],\n",
       "       [-1.143 , -0.132 , -1.3402, -1.3154],\n",
       "       [-1.3854,  0.3284, -1.3971, -1.3154],\n",
       "       [-1.5065,  0.0982, -1.2834, -1.3154],\n",
       "       [-1.0218,  1.2492, -1.3402, -1.3154],\n",
       "       [-0.5372,  1.9398, -1.1697, -1.0522]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для начала проверим, что перемножим получившиеся матрицы мы получим исходную.\n",
    "(u@D@vh)[:6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b234cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150.    , -17.6355, 130.7631, 122.6912],\n",
       "       [-17.6355, 150.    , -64.266 , -54.9189],\n",
       "       [130.7631, -64.266 , 150.    , 144.4298],\n",
       "       [122.6912, -54.9189, 144.4298, 150.    ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим перемножение матриц V@D@D@VT получим ковариационную матрицу X_.T@X_\n",
    "v@D[:4]@D[:4]@vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1113c6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150.    , -17.6355, 130.7631, 122.6912],\n",
       "       [-17.6355, 150.    , -64.266 , -54.9189],\n",
       "       [130.7631, -64.266 , 150.    , 144.4298],\n",
       "       [122.6912, -54.9189, 144.4298, 150.    ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Видно что матрицы идентичны. \n",
    "Значит сингулярное разложение может помочь нам преобразовать исходную матрицу, выполнив понижение \n",
    "размерности признаков с помощью собственных векторов матрицы ковариаций.\n",
    "\"\"\"\n",
    "\n",
    "X_.T@X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349a56cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 228.1095,  -51.7457,   15.8402,    0.8119],\n",
       "       [-117.9135, -126.5881,   -5.3797,   -0.3838],\n",
       "       [ 254.0902,   -3.3579,   -3.1287,   -2.4903],\n",
       "       [ 247.2799,   -9.1781,  -13.9626,    1.6269]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vh.T@D[:4]@D[:4]\n",
    "X_.T@X_@vh.T\n",
    "# эти матрицы, свидетельствующие о близости собственных векторов и их значений \n",
    "# с сингулярным разложением матрицы, тоже равны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e01f6",
   "metadata": {},
   "source": [
    "Выполним понижение размерности имея в виду что $W^T = V$ - составим матрицу $W^T$ из первых двух правых сингулярных векторов исходной матрицы ($X\\_$) соответствующих максимальным (по модулю) сингулярным числам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a46d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "размерность новой матрицы Z: (150, 2)\n",
      "\n",
      "Ниже первые шесть объектов новой матрицы Z:\n",
      "[[-2.2647 -0.48  ]\n",
      " [-2.081   0.6741]\n",
      " [-2.3642  0.3419]\n",
      " [-2.2994  0.5974]\n",
      " [-2.3898 -0.6468]\n",
      " [-2.0756 -1.4892]]\n",
      "\n",
      "вуаля, все сходится с тем что мы получили на уроке!\n"
     ]
    }
   ],
   "source": [
    "# выполним понижение размерности оставив только 2 признака;\n",
    "Z = X_@v[:,:2]\n",
    "print(f\"размерность новой матрицы Z: {Z.shape}\\n\")\n",
    "print(f\"Ниже первые шесть объектов новой матрицы Z:\\n\"\n",
    "     f\"{Z[:6]}\\n\"\n",
    "     f\"\\nвуаля, все сходится с тем что мы получили на уроке!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c16cf6",
   "metadata": {
    "colab_type": "text",
    "id": "b-eNSDPndfKU"
   },
   "source": [
    "#### 3. (\\*) Обучить любую модель классификации (из рассмотренных в курсе) на датасете IRIS до применения PCA и после него. Сравнить качество классификации по отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90dd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_DecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self, criterion=\"gini\", min_samples_leaf=1, max_leaf_nodes=None):\n",
    "        self.criterion = criterion\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        \n",
    "    def fit(self, data, labels, max_depth=None, subset_feats_flag=False):\n",
    "        self.max_depth = max_depth\n",
    "        quality, t, index = Node.find_best_split(self, data, labels, subset_feats_flag)\n",
    "        if (t is None) or (index is None):\n",
    "            return Leaf(data, labels)\n",
    "        true_data, false_data, true_labels, false_labels = Node.split(data, labels, index, t)\n",
    "        if (len(true_data) <= self.min_samples_leaf) or (len(false_data) <= self.min_samples_leaf):\n",
    "            return Leaf(data, labels)\n",
    "        if max_depth is not None:\n",
    "            if max_depth != 0:\n",
    "                max_d = max_depth - 1\n",
    "                true_branch = my_DecisionTreeClassifier.fit(self, true_data, true_labels, \n",
    "                                                            max_depth=max_d)\n",
    "                false_branch = my_DecisionTreeClassifier.fit(self, false_data, false_labels, \n",
    "                                                             max_depth=max_d)\n",
    "            else:\n",
    "                return Leaf(data, labels)\n",
    "        else:\n",
    "            true_branch = my_DecisionTreeClassifier.fit(self, true_data, true_labels)\n",
    "            false_branch = my_DecisionTreeClassifier.fit(self, false_data, false_labels)\n",
    "        self.dtree = Node(index, t, true_branch, false_branch)\n",
    "        return self.dtree\n",
    "    \n",
    "    @staticmethod\n",
    "    def classify_obj(obj, node):\n",
    "        if isinstance(node, Leaf):\n",
    "            answer = node.prediction\n",
    "            return answer\n",
    "        if obj[node.index] <= node.t:\n",
    "            return my_DecisionTreeClassifier.classify_obj(obj, node.true_branch)\n",
    "        else:\n",
    "            return my_DecisionTreeClassifier.classify_obj(obj, node.false_branch)\n",
    "    \n",
    "    def labels_predict(self, X, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.dtree\n",
    "        pred_vec = []\n",
    "        for obj in X:\n",
    "            pred_val = my_DecisionTreeClassifier.classify_obj(obj, tree)\n",
    "            pred_vec.append(pred_val)\n",
    "        return pred_vec\n",
    "    \n",
    "######    \n",
    "class Leaf:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.prediction = self.predict()\n",
    "    def predict(self):\n",
    "        classes = {}\n",
    "        for label in self.labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0 \n",
    "            classes[label] += 1\n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction\n",
    "    \n",
    "######    \n",
    "class Node:\n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index\n",
    "        self.t = t\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "    @staticmethod\n",
    "    def gini(labels):\n",
    "        classes = {}\n",
    "        for label in labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "        impurity = 1\n",
    "        for label in classes:\n",
    "            p = classes[label] / len(labels)\n",
    "            impurity -= p ** 2\n",
    "        return impurity\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(labels):\n",
    "        classes = {}\n",
    "        for label in labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "        entropy_index = 0\n",
    "        for label in classes:\n",
    "            p = classes[label] / len(labels)\n",
    "            entropy_index -= 0 if p == 0 else p*np.log2(p)\n",
    "        return entropy_index\n",
    "\n",
    "    @staticmethod\n",
    "    def quality(left_labels, right_labels, criterion, current_criteria):\n",
    "        weig = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "        if criterion == 'gini':\n",
    "            return current_criteria - weig * Node.gini(left_labels) \\\n",
    "                                                                - (1 - weig) * Node.gini(right_labels)\n",
    "        elif criterion == 'entropy':\n",
    "            return current_criteria - weig * Node.entropy(left_labels) \\\n",
    "                                                                - (1 - weig) * Node.entropy(right_labels)\n",
    "    \n",
    "    @staticmethod\n",
    "    def split(data, labels, index, t):\n",
    "        left = np.where(data[:, index] <= t)\n",
    "        right = np.where(data[:, index] > t)\n",
    "        true_data = data[left]\n",
    "        false_data = data[right]\n",
    "        true_labels = labels[left]\n",
    "        false_labels = labels[right]\n",
    "        return true_data, false_data, true_labels, false_labels\n",
    "    \n",
    "    def find_best_split(self, data, labels, subset_feats_flag=False):\n",
    "        if self.criterion == 'gini':\n",
    "            current_criteria = Node.gini(labels)\n",
    "        elif self.criterion == 'entropy':\n",
    "            current_criteria = Node.entropy(labels)\n",
    "        best_quality = 0\n",
    "        best_t = None\n",
    "        best_index = None\n",
    "        k = (int(np.sqrt(data.shape[1])) if subset_feats_flag else data.shape[1])\n",
    "        for index in random.sample(range(data.shape[1]), k=k):\n",
    "            t_values = np.unique([row[index] for row in data])\n",
    "            for t in t_values:\n",
    "                true_data, false_data, true_labels, false_labels = Node.split(data, labels, index, t)\n",
    "                if len(true_data) < self.min_samples_leaf or len(false_data) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                current_quality = Node.quality(true_labels, false_labels, \n",
    "                                               self.criterion, current_criteria)\n",
    "                if current_quality > best_quality:\n",
    "                    best_quality, best_t, best_index = current_quality, t, index\n",
    "        return best_quality, best_t, best_index\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_metric(actual, predicted):\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e35f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart method Accuracy metric value is: 0.98\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "y_labels = iris.target\n",
    "dt_cls = my_DecisionTreeClassifier()\n",
    "dt_cls.fit(X_, y_labels)\n",
    "pred = dt_cls.labels_predict(X_)\n",
    "print(f\"Standart method Accuracy metric value is: {dt_cls.dtree.accuracy_metric(y_labels, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52542108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA method Accuracy metric value is: 0.98\n"
     ]
    }
   ],
   "source": [
    "dt_cls_PCA = my_DecisionTreeClassifier()\n",
    "dt_cls_PCA.fit(Z, y_labels)\n",
    "pred_Z = dt_cls_PCA.labels_predict(Z)\n",
    "print(f\"PCA method Accuracy metric value is: {Node.accuracy_metric(y_labels, pred_Z)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bb59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_RandomForestClassifier(my_DecisionTreeClassifier):\n",
    "    \n",
    "    def __init__(self, n_trees=10):\n",
    "        self.n_trees = n_trees\n",
    "\n",
    "    def get_bootstrap(self, X, y, N=None):\n",
    "        if N is None: N = self.n_trees\n",
    "        random.seed(29)\n",
    "        n_samples = X.shape[0]\n",
    "        bootstrap_set = []\n",
    "        for _ in range(N):\n",
    "            indices = random.choices(range(n_samples), k=n_samples)\n",
    "            bootstrap_set.append((X[indices], y[indices]))\n",
    "        return bootstrap_set\n",
    "    \n",
    "    def oob_bootstrap(self, X, y, N=None):\n",
    "        if N is None: N = self.n_trees\n",
    "        random.seed(29)\n",
    "        n_samples = X.shape[0]\n",
    "        bootstrap_set = []\n",
    "        oob_set = []\n",
    "        oob_inv_ind = []\n",
    "        for _ in range(N):\n",
    "            indices = random.choices(range(n_samples), k=n_samples)\n",
    "            inv_indices = [val for val in range(n_samples) if not val in set(indices)]\n",
    "            oob_set.append((X[inv_indices], y[inv_indices]))\n",
    "            oob_inv_ind.append(inv_indices)\n",
    "            bootstrap_set.append((X[indices], y[indices]))\n",
    "        self.oob_inv_ind = oob_inv_ind\n",
    "        self.oob_set = oob_set\n",
    "        return bootstrap_set\n",
    "    \n",
    "    \"\"\"данный метод не потребовался; я внес изменения в надкласс my_DecisionTreeClassifier, \n",
    "       а конкретно в метод find_best_split();\"\"\"\n",
    "    #     @staticmethod\n",
    "    #     def get_feats_subset(num_feats):\n",
    "    #         num_subfeats = int(np.sqrt(num_feats))\n",
    "    #         subfeat_inds = random.sample(range(num_feats), k=num_subfeats)\n",
    "    #         return subfeat_inds\n",
    "    \n",
    "    def randomforest_build(self, X, y, n_trees=None):\n",
    "        if n_trees is None: n_trees = self.n_trees\n",
    "        forest = []\n",
    "        bs_set = my_RandomForestClassifier.oob_bootstrap(self, X, y, n_trees)\n",
    "        for bs_X, bs_y in bs_set:\n",
    "            cls = my_DecisionTreeClassifier()\n",
    "            forest.append(cls.fit(bs_X, bs_y, subset_feats_flag=True))\n",
    "        return forest\n",
    "    \n",
    "    def tree_vote(self, forest, data):\n",
    "        pred = []\n",
    "        for tree in forest:\n",
    "            pred.append(my_DecisionTreeClassifier.labels_predict(self, data, tree))\n",
    "        pred_per_obj = list(zip(*pred))\n",
    "        voted_pred = []\n",
    "        for obj in pred_per_obj:\n",
    "            voted_pred.append(max(obj, key=obj.count))\n",
    "        return voted_pred\n",
    "    \n",
    "    def oob_vote(self, forest):\n",
    "        oob_pred = []\n",
    "        oob_set = self.oob_set\n",
    "        oob_inv_ind = self.oob_inv_ind\n",
    "        for num, tree in enumerate(forest):\n",
    "            oob_pred.append(my_DecisionTreeClassifier.labels_predict(self, oob_set[num][0], tree))\n",
    "        oob_dict_pred = {}\n",
    "        oob_dict_actu = {}\n",
    "        for pos, p in enumerate(oob_pred):\n",
    "            for num, val in enumerate(p):\n",
    "                if oob_inv_ind[pos][num] not in oob_dict_pred:\n",
    "                    oob_dict_pred[oob_inv_ind[pos][num]] = [val]\n",
    "                    oob_dict_actu[oob_inv_ind[pos][num]] = oob_set[pos][1][num]\n",
    "                else:\n",
    "                    oob_dict_pred[oob_inv_ind[pos][num]].append(val)\n",
    "        for k, v in oob_dict_pred.items():\n",
    "            oob_dict_pred[k] = max(v, key=v.count)\n",
    "        oob_pred_list = list(oob_dict_pred.values())\n",
    "        oob_actu_list = list(oob_dict_actu.values())\n",
    "        return oob_actu_list, oob_pred_list  # oob_dict_pred, oob_dict_actu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df35f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart method accuracy method is: 0.9800\n",
      "Accuracy check: 0.9800\n"
     ]
    }
   ],
   "source": [
    "rf_cls = my_RandomForestClassifier()\n",
    "forest1 = rf_cls.randomforest_build(X_, y_labels)\n",
    "pred1 = rf_cls.tree_vote(forest1, X_)\n",
    "accuracy1 = forest1[0].accuracy_metric(y_labels, pred1)\n",
    "print(f\"Standart method accuracy method is: {accuracy1:.4f}\")\n",
    "print(f\"Accuracy check: {Node.accuracy_metric(y_labels, pred1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6aff3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA method accuracy value is: 0.9733\n",
      "Accuracy check: 0.9733\n"
     ]
    }
   ],
   "source": [
    "rf_cls_PCA = my_RandomForestClassifier()\n",
    "forest2 = rf_cls_PCA.randomforest_build(Z, y_labels)\n",
    "pred2 = rf_cls.tree_vote(forest2, Z)\n",
    "accuracy2 = forest2[0].accuracy_metric(y_labels, pred2)\n",
    "print(f\"PCA method accuracy value is: {accuracy2:.4f}\")\n",
    "print(f\"Accuracy check: {Node.accuracy_metric(y_labels, pred2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e005e316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for c, m, i in zip(\"bgr\", \"osP\", range(3)):\n",
    "    plt.scatter(Z[y_labels==i, 0], Z[y_labels==i, 1], c=c, marker=m)\n",
    "plt.xlabel(\"feature1\")\n",
    "plt.ylabel(\"feature2\")\n",
    "plt.title(\"PCA decomposition method\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25159082",
   "metadata": {},
   "source": [
    "<font color=\"yellow\" size=3.5 face=\"serif\">Вывод: Как видно по метрикам разница в использовании данных до и после понижения размерности методом PCA - почти (значимо) не отличаются.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9bc36",
   "metadata": {},
   "source": [
    "#### 4. Принять участие в соревнованиях я к сожалению не смог(("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
